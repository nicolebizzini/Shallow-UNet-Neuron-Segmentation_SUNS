{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Active dataset: data\n",
            "Data folder: /gpfs/data/shohamlab/nicole/code/Shallow-UNet-Neuron-Segmentation_SUNS/demo/data\n",
            "GT Masks directory: /gpfs/data/shohamlab/nicole/code/Shallow-UNet-Neuron-Segmentation_SUNS/demo/data/GT Masks\n",
            "Found 8 FinalMasks files\n",
            "Processing: FinalMasks_YST_part22.mat\n",
            "  Shape: (79, 120, 88) (ncells=79, Ly=120, Lx=88)\n",
            "  Created sparse version: FinalMasks_YST_part22_sparse.mat\n",
            "Processing: FinalMasks_YST_part11.mat\n",
            "  Shape: (75, 120, 88) (ncells=75, Ly=120, Lx=88)\n",
            "  Created sparse version: FinalMasks_YST_part11_sparse.mat\n",
            "Processing: FinalMasks_YST_part12.mat\n",
            "  Shape: (99, 120, 88) (ncells=99, Ly=120, Lx=88)\n",
            "  Created sparse version: FinalMasks_YST_part12_sparse.mat\n",
            "Processing: FinalMasks_YST_part21.mat\n",
            "  Shape: (89, 120, 88) (ncells=89, Ly=120, Lx=88)\n",
            "  Created sparse version: FinalMasks_YST_part21_sparse.mat\n",
            "Sparse GT generation completed!\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Generate Sparse GT Masks\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import h5py\n",
        "from scipy.io import savemat, loadmat\n",
        "from scipy import sparse\n",
        "import glob\n",
        "\n",
        "# Add the suns directory to the path to import config\n",
        "sys.path.insert(0, os.path.join(os.getcwd(), 'suns'))\n",
        "from config import DATAFOLDER_SETS, ACTIVE_EXP_SET\n",
        "\n",
        "print(f\"Active dataset: {ACTIVE_EXP_SET}\")\n",
        "print(f\"Data folder: {DATAFOLDER_SETS[ACTIVE_EXP_SET]}\")\n",
        "\n",
        "# Set the path of the 'GT Masks' folder, which contains the manual labels in 3D arrays.\n",
        "# Use config to get the active dataset path\n",
        "data_folder = DATAFOLDER_SETS[ACTIVE_EXP_SET]\n",
        "dir_Masks = os.path.join(data_folder, 'GT Masks')\n",
        "\n",
        "print(f\"GT Masks directory: {dir_Masks}\")\n",
        "\n",
        "# %%\n",
        "dir_all = glob.glob(os.path.join(dir_Masks,'*FinalMasks*.mat'))\n",
        "print(f\"Found {len(dir_all)} FinalMasks files\")\n",
        "\n",
        "for path_name in dir_all:\n",
        "    file_name = os.path.split(path_name)[1]\n",
        "    if '_sparse' not in file_name:\n",
        "        print(f\"Processing: {file_name}\")\n",
        "        try: # If file_name is saved in '-v7.3' format\n",
        "            mat = h5py.File(path_name,'r')\n",
        "            FinalMasks = np.array(mat['FinalMasks']).astype('bool')\n",
        "            mat.close()\n",
        "        except OSError: # If file_name is not saved in '-v7.3' format\n",
        "            mat = loadmat(path_name)\n",
        "            FinalMasks = np.array(mat[\"FinalMasks\"]).transpose([2,1,0]).astype('bool')\n",
        "\n",
        "        (ncells,Ly,Lx) = FinalMasks.shape\n",
        "        print(f\"  Shape: {FinalMasks.shape} (ncells={ncells}, Ly={Ly}, Lx={Lx})\")\n",
        "        GTMasks_2=sparse.coo_matrix(FinalMasks.reshape(ncells,Lx*Ly).T)\n",
        "        savemat(os.path.join(path_name[:-4]+'_sparse.mat'), \\\n",
        "            {'GTMasks_2':GTMasks_2}, do_compression=True)\n",
        "        print(f\"  Created sparse version: {file_name[:-4]+'_sparse.mat'}\")\n",
        "\n",
        "print(\"Sparse GT generation completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Train CNN and Optimize Parameters\n",
        "import sys\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import math\n",
        "import h5py\n",
        "from scipy.io import savemat, loadmat\n",
        "import multiprocessing as mp\n",
        "\n",
        "sys.path.insert(0, 'suns') # the path containing \"suns\" folder\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # Set which GPU to use. '-1' uses only CPU.\n",
        "\n",
        "from PreProcessing.preprocessing_functions import preprocess_video, find_dataset\n",
        "from PreProcessing.generate_masks import generate_masks\n",
        "from train_CNN_params import train_CNN, parameter_optimization_cross_validation\n",
        "from config import DATAFOLDER_SETS, ACTIVE_EXP_SET, EXP_ID_SETS, OUTPUT_FOLDER, RATE_HZ, MAG\n",
        "\n",
        "import tensorflow as tf\n",
        "tf_version = int(tf.__version__[0])\n",
        "if tf_version == 1:\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config = config)\n",
        "else: # tf_version == 2:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "print(f\"Starting CNN training for dataset: {ACTIVE_EXP_SET}\")\n",
        "print(f\"Experiment IDs: {EXP_ID_SETS[ACTIVE_EXP_SET]}\")\n",
        "print(f\"Data folder: {DATAFOLDER_SETS[ACTIVE_EXP_SET]}\")\n",
        "print(f\"Output folder: {OUTPUT_FOLDER[ACTIVE_EXP_SET]}\")\n",
        "print(f\"Rate: {RATE_HZ[ACTIVE_EXP_SET]} Hz, Magnification: {MAG[ACTIVE_EXP_SET]}\")\n",
        "\n",
        "#-------------- Start user-defined parameters --------------#\n",
        "# %% set folders\n",
        "# file names of the \".h5\" files storing the raw videos. \n",
        "list_Exp_ID = EXP_ID_SETS[ACTIVE_EXP_SET]\n",
        "# folder of the raw videos\n",
        "dir_video = DATAFOLDER_SETS[ACTIVE_EXP_SET] \n",
        "# folder of the \".mat\" files stroing the GT masks in sparse 2D matrices. 'FinalMasks_' is a prefix of the file names. \n",
        "dir_GTMasks = os.path.join(dir_video, 'GT Masks', 'FinalMasks_') \n",
        "\n",
        "# %% set video parameters\n",
        "rate_hz = RATE_HZ[ACTIVE_EXP_SET] # frame rate of the video\n",
        "Mag = MAG[ACTIVE_EXP_SET] # spatial magnification compared to ABO videos (0.785 um/pixel). # Mag = 0.785 / pixel_size\n",
        "\n",
        "# %% set the range of post-processing hyper-parameters to be optimized in\n",
        "# minimum area of a neuron (unit: pixels in ABO videos). must be in ascend order\n",
        "list_minArea = list(range(30,85,5)) \n",
        "# average area of a typical neuron (unit: pixels in ABO videos)\n",
        "list_avgArea = [177] \n",
        "# uint8 threshould of probablity map (uint8 variable, = float probablity * 256 - 1)\n",
        "list_thresh_pmap = list(range(130,235,10))\n",
        "# threshold to binarize the neuron masks. For each mask, \n",
        "# values higher than \"thresh_mask\" times the maximum value of the mask are set to one.\n",
        "thresh_mask = 0.5\n",
        "# maximum COM distance of two masks to be considered the same neuron in the initial merging (unit: pixels in ABO videos)\n",
        "thresh_COM0 = 2\n",
        "# maximum COM distance of two masks to be considered the same neuron (unit: pixels in ABO videos)\n",
        "list_thresh_COM = list(np.arange(4, 9, 1)) \n",
        "# minimum IoU of two masks to be considered the same neuron\n",
        "list_thresh_IOU = [0.5] \n",
        "# minimum consecutive number of frames of active neurons\n",
        "list_cons = list(range(1, 8, 1)) \n",
        "\n",
        "# %% set pre-processing parameters\n",
        "gauss_filt_size = 50*Mag # standard deviation of the spatial Gaussian filter in pixels\n",
        "num_median_approx = 1000 # number of frames used to caluclate median and median-based standard deviation\n",
        "filename_TF_template = 'demo/YST_spike_tempolate.h5' # File name storing the temporal filter kernel\n",
        "h5f = h5py.File(filename_TF_template,'r')\n",
        "Poisson_filt = np.array(h5f['filter_tempolate']).squeeze().astype('float32')\n",
        "h5f.close()\n",
        "Poisson_filt = Poisson_filt[Poisson_filt>np.exp(-1)] # temporal filter kernel\n",
        "Poisson_filt = Poisson_filt/Poisson_filt.sum()\n",
        "\n",
        "# %% set training parameters\n",
        "thred_std = 3 # SNR threshold used to determine when neurons are active\n",
        "num_train_per = 2400 # Number of frames per video used for training \n",
        "NO_OF_EPOCHS = 200 # Number of epoches used for training \n",
        "batch_size_eval = 100 # batch size in CNN inference\n",
        "list_thred_ratio = [thred_std] # A list of SNR threshold used to determine when neurons are active.\n",
        "\n",
        "# %% Set processing options\n",
        "useSF=False # True if spatial filtering is used in pre-processing.\n",
        "useTF=True # True if temporal filtering is used in pre-processing.\n",
        "useSNR=True # True if pixel-by-pixel SNR normalization filtering is used in pre-processing.\n",
        "med_subtract=False # True if the spatial median of every frame is subtracted before temporal filtering.\n",
        "prealloc=False # True if pre-allocate memory space for large variables in pre-processing. \n",
        "useWT=False # True if using additional watershed\n",
        "load_exist=False # True if using temp files already saved in the folders\n",
        "use_validation = True # True to use a validation set outside the training set\n",
        "useMP = True # True to use multiprocessing to speed up\n",
        "BATCH_SIZE = 20 # Batch size for training \n",
        "# Cross-validation strategy. Can be \"leave_one_out\", \"train_1_test_rest\", or \"use_all\"\n",
        "cross_validation = \"leave_one_out\"\n",
        "Params_loss = {'DL':1, 'BCE':20, 'FL':0, 'gamma':1, 'alpha':0.25} # Parameters of the loss function\n",
        "#-------------- End user-defined parameters --------------#\n",
        "\n",
        "dir_parent = os.path.join(dir_video, OUTPUT_FOLDER[ACTIVE_EXP_SET]) # folder to save all the processed data\n",
        "dir_network_input = os.path.join(dir_parent, 'network_input') # folder of the SNR videos\n",
        "dir_mask = os.path.join(dir_parent, 'temporal_masks({})'.format(thred_std)) # foldr to save the temporal masks\n",
        "weights_path = os.path.join(dir_parent, 'Weights') # folder to save the trained CNN\n",
        "training_output_path = os.path.join(dir_parent, 'training output') # folder to save the loss functions during training\n",
        "dir_output = os.path.join(dir_parent, 'output_masks') # folder to save the optimized hyper-parameters\n",
        "dir_temp = os.path.join(dir_parent, 'temp') # temporary folder to save the F1 with various hyper-parameters\n",
        "\n",
        "# Create directories if they don't exist\n",
        "for dir_path in [dir_network_input, weights_path, training_output_path, dir_output, dir_temp]:\n",
        "    if not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path) \n",
        "\n",
        "print(f\"Created output directories in: {dir_parent}\")\n",
        "\n",
        "# Continue with the rest of the training script...\n",
        "# (Note: This is a simplified version for testing - you may want to run the full script separately)\n",
        "print(\"CNN training setup completed! Run the full demo_train_CNN_params.py script for complete training.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: Test Batch Processing\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import h5py\n",
        "import sys\n",
        "\n",
        "from scipy.io import savemat, loadmat\n",
        "import multiprocessing as mp\n",
        "\n",
        "sys.path.insert(0, 'suns') # the path containing \"suns\" folder\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0' # Set which GPU to use. '-1' uses only CPU.\n",
        "\n",
        "from PostProcessing.evaluate import GetPerformance_Jaccard_2\n",
        "from run_suns import suns_batch\n",
        "from config import DATAFOLDER_SETS, ACTIVE_EXP_SET, EXP_ID_SETS, OUTPUT_FOLDER, RATE_HZ, MAG\n",
        "\n",
        "import tensorflow as tf\n",
        "tf_version = int(tf.__version__[0])\n",
        "if tf_version == 1:\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    sess = tf.Session(config = config)\n",
        "else: # tf_version == 2:\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "print(f\"Starting batch testing for dataset: {ACTIVE_EXP_SET}\")\n",
        "print(f\"Experiment IDs: {EXP_ID_SETS[ACTIVE_EXP_SET]}\")\n",
        "print(f\"Data folder: {DATAFOLDER_SETS[ACTIVE_EXP_SET]}\")\n",
        "print(f\"Output folder: {OUTPUT_FOLDER[ACTIVE_EXP_SET]}\")\n",
        "\n",
        "#-------------- Start user-defined parameters --------------#\n",
        "# %% set folders\n",
        "# file names of the \".h5\" files storing the raw videos. \n",
        "list_Exp_ID = EXP_ID_SETS[ACTIVE_EXP_SET]\n",
        "# folder of the raw videos\n",
        "dir_video = DATAFOLDER_SETS[ACTIVE_EXP_SET] \n",
        "# folder of the \".mat\" files stroing the GT masks in sparse 2D matrices. 'FinalMasks_' is a prefix of the file names. \n",
        "dir_GTMasks = os.path.join(dir_video, 'GT Masks', 'FinalMasks_') \n",
        "\n",
        "# %% set video parameters\n",
        "rate_hz = RATE_HZ[ACTIVE_EXP_SET] # frame rate of the video\n",
        "Mag = MAG[ACTIVE_EXP_SET] # spatial magnification compared to ABO videos (0.785 um/pixel). # Mag = 0.785 / pixel_size\n",
        "\n",
        "# %% set pre-processing parameters\n",
        "gauss_filt_size = 50*Mag # standard deviation of the spatial Gaussian filter in pixels\n",
        "num_median_approx = 1000 # number of frames used to caluclate median and median-based standard deviation\n",
        "filename_TF_template = 'demo/YST_spike_tempolate.h5' # File name storing the temporal filter kernel\n",
        "h5f = h5py.File(filename_TF_template,'r')\n",
        "Poisson_filt = np.array(h5f['filter_tempolate']).squeeze().astype('float32')\n",
        "h5f.close()\n",
        "Poisson_filt = Poisson_filt[Poisson_filt>np.exp(-1)] # temporal filter kernel\n",
        "Poisson_filt = Poisson_filt/Poisson_filt.sum()\n",
        "\n",
        "# %% Set processing options\n",
        "useSF=False # True if spatial filtering is used in pre-processing.\n",
        "useTF=True # True if temporal filtering is used in pre-processing.\n",
        "useSNR=True # True if pixel-by-pixel SNR normalization filtering is used in pre-processing.\n",
        "med_subtract=False # True if the spatial median of every frame is subtracted before temporal filtering.\n",
        "prealloc=True # True if pre-allocate memory space for large variables in pre-processing. \n",
        "batch_size_eval = 200 # batch size in CNN inference\n",
        "useWT=False # True if using additional watershed\n",
        "display=True # True if display information about running time \n",
        "#-------------- End user-defined parameters --------------#\n",
        "\n",
        "dir_parent = os.path.join(dir_video, OUTPUT_FOLDER[ACTIVE_EXP_SET]) # folder to save all the processed data\n",
        "dir_output = os.path.join(dir_parent, 'output_masks') # folder to save the segmented masks and the performance scores\n",
        "dir_params = os.path.join(dir_parent, 'output_masks') # folder of the optimized hyper-parameters\n",
        "weights_path = os.path.join(dir_parent, 'Weights') # folder of the trained CNN\n",
        "\n",
        "print(f\"Output directories:\")\n",
        "print(f\"  Parent: {dir_parent}\")\n",
        "print(f\"  Output: {dir_output}\")\n",
        "print(f\"  Params: {dir_params}\")\n",
        "print(f\"  Weights: {weights_path}\")\n",
        "\n",
        "# Check if required files exist\n",
        "print(f\"\\nChecking for required files:\")\n",
        "for exp_id in list_Exp_ID:\n",
        "    video_file = os.path.join(dir_video, f\"{exp_id}.h5\")\n",
        "    gt_file = os.path.join(dir_GTMasks, f\"{exp_id}_sparse.mat\")\n",
        "    print(f\"  {exp_id}:\")\n",
        "    print(f\"    Video: {video_file} - {'✓' if os.path.exists(video_file) else '✗'}\")\n",
        "    print(f\"    GT: {gt_file} - {'✓' if os.path.exists(gt_file) else '✗'}\")\n",
        "\n",
        "# Check for trained models\n",
        "print(f\"\\nChecking for trained models in {weights_path}:\")\n",
        "if os.path.exists(weights_path):\n",
        "    model_files = [f for f in os.listdir(weights_path) if f.endswith('.h5')]\n",
        "    print(f\"  Found {len(model_files)} model files: {model_files}\")\n",
        "else:\n",
        "    print(f\"  Weights directory does not exist - training required first\")\n",
        "\n",
        "# Check for optimization results\n",
        "print(f\"\\nChecking for optimization results in {dir_params}:\")\n",
        "if os.path.exists(dir_params):\n",
        "    opt_files = [f for f in os.listdir(dir_params) if f.startswith('Optimization_Info')]\n",
        "    print(f\"  Found {len(opt_files)} optimization files: {opt_files}\")\n",
        "else:\n",
        "    print(f\"  Params directory does not exist - training required first\")\n",
        "\n",
        "print(\"\\nBatch testing setup completed! Run the full demo_test_batch.py script for complete testing.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Switch Dataset Configuration\n",
        "# To switch between datasets, modify the ACTIVE_EXP_SET in config.py\n",
        "# Then restart the kernel and run all cells again\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, 'suns')\n",
        "from config import DATAFOLDER_SETS, ACTIVE_EXP_SET, EXP_ID_SETS, OUTPUT_FOLDER, RATE_HZ, MAG\n",
        "\n",
        "print(\"Current Configuration:\")\n",
        "print(f\"  Active Dataset: {ACTIVE_EXP_SET}\")\n",
        "print(f\"  Data Folder: {DATAFOLDER_SETS[ACTIVE_EXP_SET]}\")\n",
        "print(f\"  Experiment IDs: {EXP_ID_SETS[ACTIVE_EXP_SET]}\")\n",
        "print(f\"  Output Folder: {OUTPUT_FOLDER[ACTIVE_EXP_SET]}\")\n",
        "print(f\"  Frame Rate: {RATE_HZ[ACTIVE_EXP_SET]} Hz\")\n",
        "print(f\"  Magnification: {MAG[ACTIVE_EXP_SET]}\")\n",
        "\n",
        "print(\"\\nAvailable Datasets:\")\n",
        "for dataset in DATAFOLDER_SETS.keys():\n",
        "    print(f\"  {dataset}:\")\n",
        "    print(f\"    Data: {DATAFOLDER_SETS[dataset]}\")\n",
        "    print(f\"    Experiments: {EXP_ID_SETS[dataset]}\")\n",
        "    print(f\"    Output: {OUTPUT_FOLDER[dataset]}\")\n",
        "    print(f\"    Rate: {RATE_HZ[dataset]} Hz\")\n",
        "    print(f\"    Mag: {MAG[dataset]}\")\n",
        "\n",
        "print(\"\\nTo switch datasets:\")\n",
        "print(\"1. Edit suns/config.py\")\n",
        "print(\"2. Change ACTIVE_EXP_SET = 'data' to ACTIVE_EXP_SET = 'line3_dataset' (or vice versa)\")\n",
        "print(\"3. Restart kernel and run all cells\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
